# -*- coding: utf-8 -*-
"""End member estimation from DLR_HySU dataset using trained model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/147-l48j-RLwdF268oCHpGdUwE52Rof2u
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import os
from os import listdir
import glob
import scipy.io as sio
import keras
from sklearn.utils import resample
from scipy.sparse import coo_matrix
from keras.models import Sequential
from keras.layers import Dense, Flatten, Convolution1D, Dropout, Activation
from keras.optimizers import SGD
from keras.initializers import random_uniform
from sklearn.model_selection import train_test_split
#from keras.layers.convolutional import Conv1D
from keras.models import Sequential
from keras.layers import Conv1D, Dense, MaxPool1D, Flatten, Input
from keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import layers, models
from sklearn.metrics import confusion_matrix, classification_report
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE

path="/content/drive/MyDrive/dataset em wise/"
import numpy as np
X=np.load(path+'X.npy')
y=np.load(path+'Y.npy')
y=y.astype('int')
nClass=6

X.shape
plt.plot(X[45,0:10])

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import os
from os import listdir
import glob
import scipy.io as sio
import keras
from sklearn.utils import resample
from scipy.sparse import coo_matrix
from keras.models import Sequential
from keras.layers import Dense, Flatten, Convolution1D, Dropout, Activation
from keras.optimizers import SGD
from keras.initializers import random_uniform
from sklearn.model_selection import train_test_split
#from keras.layers.convolutional import Conv1D
from keras.models import Sequential
from keras.layers import Conv1D, Dense, MaxPool1D, Flatten, Input
from keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import layers, models
from sklearn.metrics import confusion_matrix, classification_report
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE

# Define the 1D CNN model
def create_cnn_model(input_shape, num_classes):
    model = models.Sequential([
        layers.Conv1D(32, kernel_size=3, activation='tanh', input_shape=input_shape),
        layers.MaxPooling1D(pool_size=2),
        layers.Conv1D(32, kernel_size=3, activation='tanh', input_shape=input_shape),
        layers.MaxPooling1D(pool_size=2),
        layers.Conv1D(48, kernel_size=3, activation='tanh', input_shape=input_shape),
        layers.MaxPooling1D(pool_size=2),
        layers.Conv1D(48, kernel_size=3, activation='tanh'),
        layers.MaxPooling1D(pool_size=2),
        # layers.Conv1D(32, kernel_size=3, activation='tanh'),
        # layers.MaxPooling1D(pool_size=2),
        # layers.Conv1D(16, kernel_size=3, activation='tanh'),
        # layers.MaxPooling1D(pool_size=2),
        layers.Flatten(),
        layers.Dense(64, activation='tanh'),
        layers.Dropout(0.25),
        layers.Dense(32, activation='tanh'),
        layers.Dropout(0.25),
        # layers.Dense(32, activation='tanh'),
        # layers.Dropout(0.25),
        layers.Dense(num_classes, activation='softmax')
    ])
    return model

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

# Create the CNN model
sequence_length=X.shape[1]
num_classes=6
input_shape = (sequence_length, 1)
model = create_cnn_model(input_shape, num_classes)

model.summary()

# Compile the model
model.compile(optimizer='Adam', loss='crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
pred=model.predict(X_test)
pred = np.argmax(pred,axis=1)

history = model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=1)

import pickle
pickle.dump(model, open('CNN_nEm_Est', 'wb'))

#Compute Classwise Precision Recall, f1, Accuracy
cm = confusion_matrix(y_test, pred)
cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
cm.diagonal()

from sklearn.metrics import precision_score, f1_score, recall_score, classification_report
f1=f1_score(y_test,pred, average='macro')
prec=precision_score(y_test,pred,average='macro')
rec=recall_score(y_test,pred,average='macro')
target_names = ['nEm3', 'nEm4', 'nEm5', 'nEm6', 'nEm8','nEm9']
print(classification_report(y_test,pred, target_names=target_names,digits=4))
print(f1,prec,rec)

# serialize model to JSON
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model.weights.h5")

from keras.models import model_from_json

# load json and create model
json_file = open('model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("model.weights.h5")

# File path
hdr_file_path = '/content/DLR_HySU_HS_full.hdr'

# Read the .hdr file
hdr_data = {}
with open(hdr_file_path, 'r') as file:
    for line in file:
        # Parse lines containing key-value pairs
        if '=' in line:
            key, value = line.strip().split('=', 1)
            hdr_data[key.strip()] = value.strip()

# Display parsed header data
print("HDR Metadata:", hdr_data)

# Extract rows, columns, and bands
rows = int(hdr_data.get('lines', 0))     # Number of rows
cols = int(hdr_data.get('samples', 0))  # Number of columns
bands = int(hdr_data.get('bands', 0))   # Number of spectral bands

print(f"Rows: {rows}, Columns: {cols}, Bands: {bands}")

tif_file_path = "/content/DLR_HySU_HS_full.tif"
!pip install rasterio
import rasterio
import numpy as np

# Load HSI data from .tif file
with rasterio.open(tif_file_path) as src:
    hsi_data = src.read()  # (Bands, Height, Width)

# Transpose to (Height, Width, Bands)
hsi_data = np.transpose(hsi_data, (1, 2, 0))

print(f"HSI Data Shape: {hsi_data.shape}")
Z=hsi_data
Z.shape

[n1,n2,n3]=np.shape(Z)
ZRes=np.reshape(Z,(n1*n2,n3))
ZRes.shape

ZData=np.transpose(ZRes)
ZData.shape

# Calculate the covariance matrix
ZCOV = np.cov(ZData)
# Compute eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(ZCOV)
# Sort eigenvalues in descending order
sorted_eigenvalues = np.sort(eigenvalues)[::-1]
# Extract the first 100 eigenvalues
Top100EigenValues = sorted_eigenvalues[:100]
# Save the top 100 eigenvalues to a .mat file
i = 1  # Set your desired index value
filename = f"{i}.mat"
from scipy.io import savemat # Import savemat from scipy.io
savemat(filename, {'Top100EigenValues': Top100EigenValues})

print(f"Top 100 eigenvalues saved to {filename}")

from scipy.io import loadmat

# Load the .mat file
data = loadmat('1.mat')

# Extract the top 100 eigenvalues
Top100EigenValues = data['Top100EigenValues']

X=np.transpose(Top100EigenValues)

print(f"Shape of loaded data: {X.shape}")

from scipy.io import loadmat

# Load the .mat file
data = loadmat('1.mat')

# Extract the top 100 eigenvalues
Top100EigenValues = data['Top100EigenValues']

# Convert to real values if they are complex
if np.iscomplexobj(Top100EigenValues):
    Top100EigenValues = Top100EigenValues.real

X = np.transpose(Top100EigenValues)

print(f"Shape of loaded data: {X.shape}")

# Reshape X to have the expected shape (num_samples, sequence_length, num_features)
X = X.reshape(1, X.shape[0], 1)  # Reshape to (1, 100, 1)
# Ensure X is of type float32
X = X.astype(np.float32)

# Make predictions
predictions = loaded_model.predict(X)

X.shape

predictions.argmax()

from sklearn.decomposition import PCA
import numpy as np
import scipy.io as sio

# Load the Urban image data again
data = sio.loadmat('/content/dc_endmembers.mat')

Z = data['X']
# Check the dimensions of Z
if Z.ndim == 3:
    rows, cols, bands = Z.shape
    data_reshaped = Z.reshape(-1, bands)
    # Apply PCA
    pca = PCA(n_components=3)
    data_pca = pca.fit_transform(data_reshaped)
    # Reshape PCA results back to spatial dimensions
    data_pca_image = data_pca.reshape(rows, cols, -1)
else:
    # Apply PCA directly to the 2D data
    pca = PCA(n_components=3)
    data_pca = pca.fit_transform(Z)
    data_pca_image = data_pca # Assign directly for consistency

print(f"Shape of data_pca_image: {data_pca_image.shape}")

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import scipy.io as sio

# Load the Urban image data again
data = sio.loadmat('/content/dc_endmembers.mat')

Z = data['X']

# Compute the similarity of each pixel with the mean spectrum
mean_spectrum = np.mean(Z, axis=0)
similarity = cosine_similarity(Z, mean_spectrum.reshape(1, -1)).flatten()

# Select the top N most pure pixels as endmembers
num_classes = 6 # Added the variable which is used in the original code
top_indices = np.argsort(-similarity)[:num_classes]  # Get indices of top-N pure pixels

#Get the shape
if Z.ndim == 3:
    rows, cols, bands = Z.shape
elif Z.ndim == 2:
    rows, cols = Z.shape

endmember_locations = [(idx // cols, idx % cols) for idx in top_indices]

# Print endmember locations
print("Endmember locations (row, col):", endmember_locations)