# -*- coding: utf-8 -*-
"""COMPARATIVE PERFORMANCE ASSESSMENT OF THE STATE-OF-THE-ART METHODS including Hysime,HFC-VD,GAP-VD,RF,DT,XGBOOST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f6Jw5p2qmlCgPq1m5C1_R1Hu8WEyS0xV
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
# Import All Libraries
import os
import scipy.io
from scipy.io import loadmat
import numpy as np
import matplotlib.pyplot as plt

path="/content/drive/MyDrive/dataset em wise/nem=3"
dir_list1 = os.listdir(path)
nEvals=100
nSampC1=len(dir_list1)
xC1=np.zeros([nSampC1,nEvals])
for i in range(nSampC1):
  v1 = scipy.io.loadmat(path + '/' + dir_list1[i])
  v=v1['LambdaVal']
  v=np.squeeze(v[0:nEvals])
  xC1[i,:]=v

np.shape(xC1)

path="/content/drive/MyDrive/dataset em wise/nem=4"
dir_list2 = os.listdir(path)
nSampC2=len(dir_list2)
xC2=np.zeros([nSampC2,nEvals])
for i in range(nSampC2):
  v1 = scipy.io.loadmat(path + '/' + dir_list2[i])
  v=v1['LambdaVal']
  v=np.squeeze(v[0:nEvals])
  xC2[i,:]=v

np.shape(xC2)

path="/content/drive/MyDrive/dataset em wise/nem=5"
dir_list3 = os.listdir(path)
nSampC3=len(dir_list3)
xC3=np.zeros([nSampC3,nEvals])
for i in range(nSampC3):
  v1 = scipy.io.loadmat(path + '/' + dir_list3[i])
  v=v1['LambdaVal']
  v=np.squeeze(v[0:nEvals])
  xC3[i,:]=v

np.shape(xC3)

path="/content/drive/MyDrive/dataset em wise/nem=6"
dir_list4 = os.listdir(path)
nSampC4=len(dir_list4)
xC4=np.zeros([nSampC4,nEvals])
for i in range(nSampC4):
  v1 = scipy.io.loadmat(path + '/' + dir_list4[i])
  v=v1['LambdaVal']
  v=np.squeeze(v[0:nEvals])
  xC4[i,:]=v

np.shape(xC4)

path="/content/drive/MyDrive/dataset em wise/nem=8"
dir_list5 = os.listdir(path)
nSampC5=len(dir_list5)
xC5=np.zeros([nSampC5,nEvals])
for i in range(nSampC5):
  v1 = scipy.io.loadmat(path + '/' + dir_list5[i])
  v=v1['LambdaVal']
  v=np.squeeze(v[0:nEvals])
  xC5[i,:]=v

np.shape(xC5)

path="/content/drive/MyDrive/dataset em wise/nem=9"
dir_list6 = os.listdir(path)
nSampC6=len(dir_list6)
xC6=np.zeros([nSampC6,nEvals])
for i in range(nSampC6):
  v1 = scipy.io.loadmat(path + '/' + dir_list6[i])
  v=v1['LambdaVal']
  v=np.squeeze(v[0:nEvals])
  xC6[i,:]=v

np.shape(xC6)

x=np.concatenate((xC1,xC2,xC3,xC4,xC5,xC6),axis=0)
#x=np.concatenate((xC1,xC2),axis=0)
np.shape(x)
y1=np.zeros(nSampC1)
y2=np.ones(nSampC2)
y3=2*np.ones(nSampC3)
y4=3*np.ones(nSampC4)
y5=4*np.ones(nSampC5)
y6=5*np.ones(nSampC6)
y=np.concatenate((y1,y2,y3,y4,y5,y6))
#atenate((y1,y2))

np.save('Eval.npy',x)
np.save('Class.npy',y)

# Save Image
np.save('/content/drive/MyDrive/Eval.npy',x)
np.save('/content/drive/MyDrive/Class.npy',y)

x=np.load('Eval.npy')
y=np.load('Class.npy')

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize
x=normalize(x, norm='l2')
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)
np.shape(X_train)

print('size of X_train:', X_train.shape)
print('size of X_test:',  X_test.shape)
print('size of y_train:', y_train.shape)
print('size of y_test:', y_test.shape)

"""#DECISION TREE"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Create a decision tree model
dtc = DecisionTreeClassifier()

# Train the model on the training data
dtc.fit(X_train, y_train)

# Predict the class labels for the testing data
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
y_pred = dtc.predict(X_test)

# Calculate the accuracy
accuracy = np.mean(y_pred == y_test)

target_names = ['nem=3', 'nem=4', 'nem=5','nem=6', 'nem=8', 'nem=9']
#target_names = ['nem=3','nem=4']
print(classification_report(y_test, y_pred, target_names=target_names, digits=4))

import pickle

# Assume 'decision_tree_model' is your trained Decision Tree model
with open('dtc.pkl', 'wb') as model_file:
    pickle.dump(dtc, model_file)

print("Model saved successfully!")

# Load the model
with open('dtc.pkl', 'rb') as model_file:
    loaded_model = pickle.load(model_file)

print("Model loaded successfully!")

import numpy as np
from scipy.io import loadmat, savemat
from sklearn.decomposition import PCA # Import PCA from sklearn.decomposition

# Load the Urban /content/new.matimage data
data = loadmat('/content/new.mat')  # Update with your file path
Z = data['data']  # Replace 'Y' with the correct key if different

# Reshape Z to 2D if it's 3D
if Z.ndim == 3:
    Z = Z.reshape(Z.shape[0], -1)  # Reshape to (num_samples, num_features)

pca = PCA() # Now PCA is defined and can be used
pca.fit(Z)

##For dc mall data testing using pre trained decision tree###
import numpy as np
from scipy.io import loadmat, savemat
from sklearn.decomposition import PCA # Import PCA from sklearn.decomposition

# Load the Urban /content/new.matimage data
data = loadmat('/content/dc_endmembers.mat')  # Update with your file path
Z = data['X']  # Replace 'Y' with the correct key if different

ZData=np.transpose(Z)
ZData.shape

pca = PCA() # Now PCA is defined and can be used
pca.fit(ZData)

eigenvalues = pca.explained_variance_[:100]
eigenvalues.shape

eigenvalues_normalized = eigenvalues / np.max(eigenvalues)
eigenvalues_normalized.shape

with open('dtc.pkl', 'rb') as model_file:
    decision_tree_model = pickle.load(model_file)

eigenvalues_input = eigenvalues_normalized.reshape(1, -1)
eigenvalues_input.shape

predicted_endmembers = decision_tree_model.predict(eigenvalues_input)

print(f"Predicted number of endmembers: {predicted_endmembers[0]}")
#print("The value of em is:", predicted_endmembers)

plt.plot(range(1, 101), eigenvalues, marker='o')
plt.title('First 100 Eigenvalues of Urban HSI')
plt.xlabel('Index')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()

"""***********FOr decision tree UNABLE TO MEASURE PRECISION & F1 Score. its saying Classification metrics can't handle a mix of multiclass and continuous targets************* ** ***bold text***

# SOFM
"""

import numpy as np
from numpy.ma.core import ceil
from scipy.spatial import distance #distance calculation
from sklearn.preprocessing import MinMaxScaler #normalisation
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score #scoring
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from matplotlib import animation, colors

# Helper functions

# Data Normalisation
def minmax_scaler(data):
  scaler = MinMaxScaler()
  scaled = scaler.fit_transform(data)
  return scaled

# Euclidean distance
def e_distance(x,y):
  return distance.euclidean(x,y)

# Manhattan distance
def m_distance(x,y):
  return distance.cityblock(x,y)

# Best Matching Unit search
def winning_neuron(data, t, som, num_rows, num_cols):
  winner = [0,0]
  shortest_distance = np.sqrt(data.shape[1]) # initialise with max distance
  input_data = data[t]
  for row in range(num_rows):
    for col in range(num_cols):
      distance = e_distance(som[row][col], data[t])
      if distance < shortest_distance:
        shortest_distance = distance
        winner = [row,col]
  return winner

# Learning rate and neighbourhood range calculation
def decay(step, max_steps,max_learning_rate,max_m_dsitance):
  coefficient = 1.0 - (np.float64(step)/max_steps)
  learning_rate = coefficient*max_learning_rate
  neighbourhood_range = ceil(coefficient * max_m_dsitance)
  return learning_rate, neighbourhood_range

# hyperparameters
num_rows = 10
num_cols = 10
max_m_dsitance = 4
max_learning_rate = 0.5
max_steps = int(7.5*10e3)

# num_nurons = 5*np.sqrt(train_x.shape[0])
# grid_size = ceil(np.sqrt(num_nurons))
# print(grid_size)

#mian function

train_x_norm = minmax_scaler(X_train) # normalisation

# initialising self-organising map
num_dims = train_x_norm.shape[1] # numnber of dimensions in the input data
np.random.seed(40)
som = np.random.random_sample(size=(num_rows, num_cols, num_dims)) # map construction

# start training iterations
for step in range(max_steps):
  if (step+1) % 1000 == 0:
    print("Iteration: ", step+1) # print out the current iteration for every 1k
  learning_rate, neighbourhood_range = decay(step, max_steps,max_learning_rate,max_m_dsitance)

  t = np.random.randint(0,high=train_x_norm.shape[0]) # random index of traing data
  winner = winning_neuron(train_x_norm, t, som, num_rows, num_cols)
  for row in range(num_rows):
    for col in range(num_cols):
      if m_distance([row,col],winner) <= neighbourhood_range:
        som[row][col] += learning_rate*(train_x_norm[t]-som[row][col]) #update neighbour's weight

print("SOM training completed")

# collecting labels

label_data = y_train
map = np.empty(shape=(num_rows, num_cols), dtype=object)

for row in range(num_rows):
  for col in range(num_cols):
    map[row][col] = [] # empty list to store the label

for t in range(train_x_norm.shape[0]):
  if (t+1) % 1000 == 0:
    print("sample data: ", t+1)
  winner = winning_neuron(train_x_norm, t, som, num_rows, num_cols)
  map[winner[0]][winner[1]].append(label_data[t]) # label of winning neuron

# construct label map
label_map = np.zeros(shape=(num_rows, num_cols),dtype=np.int64)
for row in range(num_rows):
  for col in range(num_cols):
    label_list = map[row][col]
    if len(label_list)==0:
      label = 2
    else:
      label = max(label_list, key=label_list.count)
    label_map[row][col] = label

title = ('Iteration ' + str(max_steps))
cmap = colors.ListedColormap(['tab:green', 'tab:red', 'tab:orange'])
plt.imshow(label_map, cmap=cmap)
plt.colorbar()
plt.title(title)
plt.show()

# test data

# using the trained som, search the winning node of corresponding to the test data
# get the label of the winning node

data = minmax_scaler(X_test) # normalisation

winner_labels = []

for t in range(data.shape[0]):
 winner = winning_neuron(data, t, som, num_rows, num_cols)
 row = winner[0]
 col = winner[1]
 predicted = label_map[row][col]
 winner_labels.append(predicted)

print("Accuracy: ",accuracy_score(y_test, np.array(winner_labels)))

"""# IMPLEMENTING BNN"""

pip install torchbnn

import numpy as np
from sklearn import datasets
import torch
import torch.nn as nn
import torch.optim as optim
import torchbnn as bnn
import matplotlib.pyplot as plt
#%matplotlib inline
#1. Load Iris Data
#iris = datasets.load_iris()
#X = iris.data
#Y = iris.target
x, y = torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long()
x.shape, y.shape

model = nn.Sequential(
    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=100, out_features=4),
    nn.ReLU(),
    bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=4, out_features=100),
)
ce_loss = nn.CrossEntropyLoss()
kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)
kl_weight = 0.01
optimizer = optim.Adam(model.parameters(), lr=0.01)
kl_weight = 0.1
for step in range(3000):
    pre = model(x)
    ce = ce_loss(pre, y)
    kl = kl_loss(model)
    cost = ce + kl_weight*kl

    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

_, predicted = torch.max(pre.data, 1)
total = y.size(0)
correct = (predicted == y).sum()
print('- Accuracy: %f %%' % (100 * float(correct) / total))
print('- CE : %2.2f, KL : %2.2f' % (ce.item(), kl.item()))

"""#LOGISTIC REGRESSION"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, accuracy_score
import sklearn
lr=sklearn.linear_model.LogisticRegression(random_state=0)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
#print(f"LR F1 Score {100*f1_score(y_test, y_pred_lr, average='macro')}")
#print(f"Accuracy {100*accuracy_score(y_test, y_pred_lr)}")
#print(f"Kappa Value {100*sklearn.metrics.cohen_kappa_score(y_test, y_pred_lr)}")
# Predict the class labels for the testing data
from sklearn.metrics import classification_report
#from sklearn.metrics import accuracy_score
#y_pred = dtc.predict(X_test)

# Calculate the accuracy
#accuracy = np.mean(y_pred == y_test)

target_names = ['nem=3', 'nem=4', 'nem=5','nem=6', 'nem=8', 'nem=9']
#target_names = ['nem=3','nem=4']
print(classification_report(y_test, y_pred, target_names=target_names, digits=4))

X_test.shape

import pickle

# Assume 'decision_tree_model' is your trained Decision Tree model
with open('lr.pkl', 'wb') as model_file:
    pickle.dump(lr, model_file)

import numpy as np
from scipy.io import loadmat, savemat
from sklearn.decomposition import PCA # Import PCA from sklearn.decomposition

# Load the Urban image data
data = loadmat('/content/new.mat')  # Update with your file path
Z = data['data']  # Replace 'Y' with the correct key if different

# Reshape Z to 2D if it's 3D
if Z.ndim == 3:
    Z = Z.reshape(Z.shape[0], -1)  # Reshape to (num_samples, num_features)

pca = PCA() # Now PCA is defined and can be used
pca.fit(Z)

eigenvalues = pca.explained_variance_[:100]
eigenvalues.shape

# Load the pretrained logistic regression model
import joblib
model = joblib.load('lr.pkl')  # Replace with your model path

# Reshape eigenvalues to match the model's input shape
eigenvalues_input = eigenvalues_normalized.reshape(1, -1)  # (1, 100)

# Predict the number of endmembers
predicted_endmembers = model.predict(eigenvalues_input)

print(f"Predicted number of endmembers: {predicted_endmembers[0]}")

eigenvalues_input.shape

"""# RANDOM FOREST CLASSIFIER"""

# %% Fit blackbox model (Random Forest)
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, accuracy_score
import sklearn
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
target_names = ['nem=3', 'nem=4', 'nem=5','nem=6', 'nem=8', 'nem=9']
#target_names = ['nem=3','nem=4']
print(classification_report(y_test, y_pred, target_names=target_names, digits=4))
#print(f"F1 Score {100*f1_score(y_test, y_pred, average='macro')}")
#print(f"Accuracy {100*accuracy_score(y_test, y_pred)}")
#print(f"Kappa Value {100*sklearn.metrics.cohen_kappa_score(y_test, y_pred)}")



"""#SMOTE+RANDOM FOREST"""

from imblearn.over_sampling import SMOTE
from torch.nn.functional import normalize
from torchvision import transforms
from sklearn import preprocessing
import sklearn
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, accuracy_score
import torch
smt = SMOTE()
X_smote, y_smote = smt.fit_resample(x, y)
X_smote=preprocessing.normalize(X_smote,norm='l2')
X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.33, random_state=42)
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

target_names = ['nem=3', 'nem=4', 'nem=5','nem=6', 'nem=8', 'nem=9']
#target_names = ['nem=3','nem=4']
print(classification_report(y_test, y_pred, target_names=target_names, digits=4))

#print(f"F1 Score {100*f1_score(y_test, y_pred, average='macro')}")
#print(f"Accuracy {100*accuracy_score(y_test, y_pred)}")
#print(f"Kappa Value {100*sklearn.metrics.cohen_kappa_score(y_test, y_pred)}")

"""# XGBCLASSIFIER"""

import collections
import xgboost as xgb
from xgboost import XGBClassifier
import matplotlib.pylab as pl
model = XGBClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
predictions = [round(value) for value in y_pred]
accuracy = accuracy_score(y_test, predictions)

target_names = ['nem=3', 'nem=4', 'nem=5','nem=6', 'nem=8', 'nem=9']
#target_names = ['nem=3','nem=4']
print(classification_report(y_test, y_pred, target_names=target_names, digits=4))

#print(f"F1 Score {100*f1_score(y_test, y_pred, average='macro')}")
#print(f"Accuracy {100*accuracy_score(y_test, y_pred)}")
#print(f"Kappa Value {100*sklearn.metrics.cohen_kappa_score(y_test, y_pred)}")

"""# t-SNE PLOT"""

# tSNE Plot
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
from sklearn import manifold, datasets
from time import time
import seaborn as sns
perplexity = [5, 30, 50, 100]
tsne = manifold.TSNE(n_components=2,init="random",random_state=0,n_iter=300,)
x1=tsne.fit_transform(X_smote)

import pandas as pd
# Convert do DataFrame and plot
tsne_result_df = pd.DataFrame({'Dim1': x1[:,0],
                               'Dim2': x1[:,1],
                               'label': y_smote})
fig, ax = plt.subplots(1)
sns.scatterplot(x='Dim1', y='Dim2', hue='label', data=tsne_result_df, ax=ax,s=20)
lim = (x1.min()-.5, x1.max()+.5)
ax.set_xlim(lim)
ax.set_ylim(lim)
ax.set_aspect('equal')
ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)
fig.savefig('tsne_Eval.png', format='png', dpi=1200)

tsne_result_df