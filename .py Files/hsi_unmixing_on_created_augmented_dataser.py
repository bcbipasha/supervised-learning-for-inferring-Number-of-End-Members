# -*- coding: utf-8 -*-
"""HSI UNMIXING on created augmented dataser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10HZJ0NDU1K25W4gFfvpi0VUL3Yug4kUW
"""

from keras.utils import CustomObjectScope
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import os
from os import listdir
import glob
import scipy.io as sio
import keras
from keras.models import Sequential
from keras.layers import Dense, Flatten, Convolution1D, Dropout, Activation
from keras.optimizers import SGD
from keras.initializers import random_uniform
from sklearn.model_selection import train_test_split
#from keras.layers.convolutional import Conv1D
from keras.models import Sequential
from keras.layers import Conv1D, Dense, MaxPool1D, Flatten, Input
from keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import layers, models

path="/content/drive/MyDrive/dataset em wise/"
import numpy as np
X=np.load(path+'X.npy')
y=np.load(path+'Y.npy')
y=y.astype('int')
nClass=6

X.shape

"""AUTOENCODER"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
# X_train=tf.keras.utils.normalize(X_train,axis=1)
# X_test=tf.keras.utils.normalize(X_test,axis=1)
X_train=np.expand_dims(X_train,axis=2)
X_test=np.expand_dims(X_test,axis=2)
sequence_length=X.shape[1]
num_classes=6

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
# X_train=tf.keras.utils.normalize(X_train,axis=1)
# X_test=tf.keras.utils.normalize(X_test,axis=1)
X_train=np.expand_dims(X_train,axis=2)
X_test=np.expand_dims(X_test,axis=2)
sequence_length=X.shape[1]
num_classes=6

X.shape[1]

def create_cnn_model(input_shape, num_classes):
    model = models.Sequential([
        keras.layers.Conv2D(filters=128, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=input_shape),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPool2D(pool_size=(2,2)),
        keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding="same"),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPool2D(pool_size=(3,3)),
        keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
        keras.layers.BatchNormalization(),
        keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding="same"),
        keras.layers.BatchNormalization(),
        keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding="same"),
        keras.layers.BatchNormalization(),
        keras.layers.MaxPool2D(pool_size=(2,2)),
        keras.layers.Flatten(),
        keras.layers.Dense(1024,activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(1024,activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(10,activation='softmax')
    ])
    return model

# Define the 1D CNN model
def create_cnn_model(input_shape, num_classes):
    model = models.Sequential([
        layers.Conv1D(16, kernel_size=3, activation='tanh', input_shape=input_shape),
        layers.MaxPooling1D(pool_size=2),
        # layers.Conv1D(16, kernel_size=3, activation='tanh', input_shape=input_shape),
        # layers.MaxPooling1D(pool_size=2),
        layers.Conv1D(32, kernel_size=3, activation='tanh', input_shape=input_shape),
        layers.MaxPooling1D(pool_size=2),
        layers.Conv1D(32, kernel_size=3, activation='tanh'),
        layers.MaxPooling1D(pool_size=2),
        layers.Conv1D(64, kernel_size=3, activation='tanh'),
        layers.MaxPooling1D(pool_size=2),
        layers.Flatten(),
        layers.Dense(64, activation='tanh'),
        layers.Dropout(0.25),
        layers.Dense(num_classes, activation='softmax')
    ])
    return model

# Create the CNN model
import tensorflow.keras.models as models
from tensorflow.keras import layers
input_shape = (sequence_length, 1)
model = create_cnn_model(input_shape, num_classes)

input_shape

# Compile the model
#from tensorflow.keras.losses import CategoricalCrossentropy
#with tf.keras.utils.custom_object_scope({'crossentropy': CategoricalCrossentropy()}):

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

from tensorflow.keras.utils import plot_model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

# Train the model
history=model.fit(X_train, y_train, batch_size=32, epochs=1000, validation_data=(X_test, y_test))

X_test.shape

# serialize model to JSON
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model.weights.h5")

from keras.models import model_from_json

# load json and create model
json_file = open('model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("model.weights.h5")

loaded_model.summary()

import numpy as np
from scipy.io import loadmat, savemat

# Load the Urban image data
data = loadmat('/content/Urban_R162.mat')  # Update with your file path
Z = data['Y']  # Replace 'Y' with the correct key if different
#Z = data['data']
#Z = data['V']
#Z=data['ppF1Image']

# Calculate the covariance matrix
# Reshape the input array to 2 dimensions
Z_reshaped = Z.reshape(Z.shape[0], -1)  # Flatten the last two dimensions

# Calculate the covariance matrix
ZCOV = np.cov(Z_reshaped)

[n1,n2,n3]=np.shape(Z)
ZRes=np.reshape(Z,(n1*n2,n3))

ZData=np.transpose(ZRes)
ZData.shape

# Calculate the covariance matrix
ZCOV = np.cov(Z)
#ZCOV = np.cov(ZRes)

# Compute eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(ZCOV)

# Sort eigenvalues in descending order
sorted_eigenvalues = np.sort(eigenvalues)[::-1]

# Extract the first 100 eigenvalues
Top100EigenValues = sorted_eigenvalues[:100]

# Save the top 100 eigenvalues to a .mat file
i = 1  # Set your desired index value
filename = f"{i}.mat"
savemat(filename, {'Top100EigenValues': Top100EigenValues})

print(f"Top 100 eigenvalues saved to {filename}")

from scipy.io import loadmat

# Load the .mat file
data = loadmat('1.mat')

# Extract the top 100 eigenvalues
Top100EigenValues = data['Top100EigenValues']

X=np.transpose(Top100EigenValues)

print(f"Shape of loaded data: {X.shape}")

import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
X_train = np.random.rand(50, 100)  # 50 samples, each with 100 eigenvalues
y_train = np.random.randint(1, 4, size=50)  # Corresponding number of endmembers
#y_train =np.array([4])
# Initialize and train the decision tree regressor
tree = DecisionTreeRegressor(random_state=42)
tree.fit(X_train, y_train)

# Predict number of endmembers for Jasper Ridge
#predicted_endmembers = tree.predict(Top100EigenValues.reshape(1, -1))
#print(f"Predicted Number of Endmembers: {int(predicted_endmembers)}")
# Predict number of endmembers for Jasper Ridge
predicted_endmembers = tree.predict(Top100EigenValues.real.reshape(1, -1)) # Use only the real part of the eigenvalues
print(f"Predicted Number of Endmembers: {int(predicted_endmembers)}")

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
# Load the eigenvalues (first 100)
# Load the eigenvalues (first 100)
#eigenvalues = np.load("eigenvalues.npy")  # Replace with the actual path - This line caused the error because the file doesn't exist.
# Instead of loading from a file, we use the existing 'eigenvalues' variable:
#eigenvalues = eigenvalues[:100] # Get the first 100 eigenvalues from the existing variable.
# Extract the first 100 eigenvalues
Top100EigenValues = sorted_eigenvalues[:100]
# You'll need to create or load 'labels' appropriately:
# labels = np.load("labels.npy")  # Number of endmembers as labels - This might also cause an error if 'labels.npy' doesn't exist.
# For now, I'll create a placeholder for 'labels':
#labels = np.random.randint(0, 6, size=100) # Replace with your actual label loading or creation.
labels = np.array([4])

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, accuracy_score
import sklearn
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
#y_pred = rf.predict(X_test)
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# ipython-input-12-633fa7d8bd08
# Reshape to a single sample with 100 features (as expected by the model)
input_data = Top100EigenValues.real.reshape(1, -1)
# NO NEED to select only the first feature
#input_data = input_data[:, 0].reshape(1, -1)
predicted_endmembers = clf.predict(input_data)
print(f"Predicted Number of Endmembers: {int(predicted_endmembers)}")

X_train.shape

import collections
import xgboost as xgb
from xgboost import XGBClassifier
import matplotlib.pylab as pl
# Before fitting the XGBoost model:
y_train_modified = y_train - 1  # Shift the labels down by 1
model = XGBClassifier()
model.fit(X_train, y_train_modified)
# ipython-input-12-633fa7d8bd08
# Reshape to a single sample with 100 features (as expected by the model)
input_data = Top100EigenValues.real.reshape(1, -1)
# NO NEED to select only the first feature
#input_data = input_data[:, 0].reshape(1, -1)
predicted_endmembers = model.predict(input_data)
print(f"Predicted Number of Endmembers: {int(predicted_endmembers)}")

labels = np.repeat(labels, Top100EigenValues.shape[1])  # Repeat the label value for each eigenvalue

# Reshape Top100EigenValues to have 100 samples
X_data = Top100EigenValues.reshape(-1, 1)

# Ensure labels has the same number of samples as X_data
labels = labels[:X_data.shape[0]]  # Adjust the number of labels

# Now perform the train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_data, labels, test_size=0.2, random_state=42)

!pip install numpy scipy matplotlib scikit-learn xgboost

import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
from sklearn.decomposition import PCA
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# Initialize XGBoost classifier
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

# Train the model
xgb.fit(X_train, y_train)

# Predict and evaluate
y_pred = xgb.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
plt.plot(eigenvalues, marker='o')
plt.title('Eigenvalues of Jasper Ridge Hyperspectral Image')
plt.xlabel('Index')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()

!pip install kneed
from kneed import KneeLocator

# Find the elbow point
# Top100EigenValues is a 2D array, flatten it to 1D before passing to KneeLocator
knee_locator = KneeLocator(range(len(Top100EigenValues[0])), Top100EigenValues[0], curve="convex", direction="decreasing")
predicted_endmembers = knee_locator.elbow # Use 'elbow' instead of 'knee' to get the elbow point

# Print the predicted number of endmembers
print("Predicted Number of Endmembers:", predicted_endmembers)

from keras.models import model_from_json

# load json and create model
json_file = open('model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("model.weights.h5")

loaded_model.summary()

from scipy.io import loadmat

# Load the .mat file
data = loadmat('1.mat')

# Extract the top 100 eigenvalues
Top100EigenValues = data['Top100EigenValues']

# Convert to real values if they are complex
if np.iscomplexobj(Top100EigenValues):
    Top100EigenValues = Top100EigenValues.real

X = np.transpose(Top100EigenValues)

print(f"Shape of loaded data: {X.shape}")

# Reshape X to have the expected shape (num_samples, sequence_length, num_features)
X = X.reshape(1, X.shape[0], 1)  # Reshape to (1, 100, 1)
# Ensure X is of type float32
X = X.astype(np.float32)

# Make predictions
predictions = loaded_model.predict(X)

# Reshape X to have the expected shape (num_samples, sequence_length, num_features)
X = X.reshape(1, X.shape[0], 1)  # Reshape to (1, 100, 1)

# Make predictions
predictions = loaded_model.predict(X)

predictions.argmax()

X.shape

# evaluate the model

# evaluate the model
_, train_acc = model.evaluate(X_train, y_train, verbose=0)
_, test_acc = model.evaluate(X_test, y_test, verbose=0)

# plot loss during training
from matplotlib import pyplot
pyplot.subplot(211)
pyplot.title('Loss')
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
# plot accuracy during training
pyplot.subplot(212)
pyplot.title('Accuracy')
pyplot.plot(history.history['accuracy'], label='train')
pyplot.plot(history.history['val_accuracy'], label='test')
pyplot.legend()
pyplot.show()

# calculating metrics for this CNN model using sklearn
from sklearn.datasets import make_circles
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from keras.models import Sequential
from keras.layers import Dense
# predict probabilities for test set
yhat_probs = model.predict(X_test, verbose=0)
# predict crisp classes for test set
#yhat_classes = model.predict_classes(X_test, verbose=0)
yhat_classes = np.argmax(model.predict(X_test), axis=1)

# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(y_test, yhat_classes)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(y_test, yhat_classes,average='macro')
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(y_test, yhat_classes,average='macro')
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(y_test, yhat_classes,average='macro')
print('F1 score: %f' % f1)
# kappa
kappa = cohen_kappa_score(y_test, yhat_classes)
print('Cohens kappa: %f' % kappa)
# confusion matrix
matrix = confusion_matrix(y_test, yhat_classes)
print(matrix)

from sklearn.metrics import classification_report

y_pred = model.predict(X_test, batch_size=64, verbose=1)
y_pred_bool = np.argmax(y_pred, axis=1)

print(classification_report(y_test, y_pred_bool))